= Source Configuration

In this chapter we'll discuss how the Source instance is configured.


== Create the Source Instance

You can create a new Source instance with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @source.avro.neo4j.json
----

Let's look at the `source.avro.neo4j.json` file:

[source,json]
----
include::ROOT:example$docker-data/contrib.source.avro.neo4j.json[]
----

This will create a Kafka Connect Source instance that will send `AVRO` message over the topic named `my-topic`.
Every message in the topic will have the following structure:

[source,json]
----
{"name": <name>, "surname": <surname>, "timestamp": <timestamp>}
----

[NOTE]
====
Please check the xref::kafka-connect/configuration.adoc[Configuration Summary] for a detailed guide about the supported configuration parameters.
====

== How the Source module pushes the data to the defined Kafka topic

The source connector pulls change events from Neo4j at the internal specified with `neo4j.cdc.poll-interval`.

Assuming a new node with label `TestSource` is created with a `name` and `timestamp` properties, the following event will be published the configured topic:

[source,json]
----
{"name":{"string":"John Doe"},"timestamp":{"long":1624551349362}}
----

Since the configuration includes `neo4j.enforce.schema=true`, this means a schema is attached to each record, in case
you want to stream pure simple JSON strings just use the relative serializer with `neo4j.enforce.schema=false` with the
following output:

[source,json]
----
{"name": "John Doe", "timestamp": 1624549598834}
----
