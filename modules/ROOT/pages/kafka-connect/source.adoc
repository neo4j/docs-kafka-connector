= Source Configuration

In this chapter we'll discuss how the Source instance is configured.


== Create the Source Instance

You can create a new Source instance with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.source.avro.neo4j.json
----

Let's look at the `contrib.source.avro.neo4j.json` file:

[source,json]
----
include::ROOT:example$docker-data/contrib.source.avro.neo4j.json[]
----

This will create a Kafka Connect Source instance that will send `AVRO` message over the topic named `my-topic`.
Every message in the topic will have the following structure:

[source,json]
----
{"name": <name>, "timestamp": <timestamp>}
----

[NOTE]
====
Please check the xref::kafka-connect/configuration.adoc[Configuration Summary] for a detailed guide about the supported configuration parameters.
====

== How the Source module pushes the data to the defined Kafka topic

[NOTE]
====
Timestamps aren't necessarily unique. As a result:

- this mode can't guarantee that all updated data will be delivered. For example, if two rows share the same timestamp, and only one has been pushed to the kafka topic before a failure, the second update will be missed when the system recovers.
- you could find duplicated events into the topic because events are sent to the topic every time the condition `ts.timestamp > $lastCheck` is satisfied. To manage this duplication you can implement some sort of "records filtering" in your consumer application or leveraging the Kafka SMT (Single Message Transformation) functions. See [here] (https://docs.confluent.io/platform/current/connect/transforms/overview.html) for further details. These functions allow you to transform the records in your topic and redirect them into a new topic (the one where your consumer application is reading from).
====

We use the query provided in the `neo4j.source.query` field by polling the database every value is into the
`neo4j.streaming.poll.interval.msecs` field.

So given the JSON configuration we have that we'll perform:

[source,cypher]
----
MATCH (ts:TestSource) WHERE ts.timestamp > $lastCheck RETURN ts.name AS name, ts.timestamp AS timestamp
----

every 5000 milliseconds by publishing events like:

[source,json]
----
{"name":{"string":"John Doe"},"timestamp":{"long":1624551349362}}
----

In this case we use `neo4j.enforce.schema=true` and this means that we will attach a schema for each record, in case
you want to stream pure simple JSON strings just use the relative serializer with `neo4j.enforce.schema=false` with the
following output:

[source,json]
----
{"name": "John Doe", "timestamp": 1624549598834}
----
