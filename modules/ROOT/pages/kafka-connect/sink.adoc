= Sink Configuration

In this chapter we'll discuss how the Sink instance is configured.


== Create the Sink Instance
:environment: neo4j
:id: neo4j

Create the Sink instance:

We'll define the Sink configuration in several ways:

* by providing a Cypher template
* by ingesting the events emitted from another Neo4j instance via the Change Data Capture module
* by providing a pattern extraction to a JSON or AVRO file
* by managing a CUD file format

== Sink ingestion strategies

[NOTE]
====
**The Cypher Template strategy is the only Sink strategy that guarantees messages to be processed in the same order as they arrive in a topic.**

Other Sink strategies group messages together by type of operation, which can also be optimised into batches. 
In this case, the execution order is the following:

. All `MERGE` operations on nodes
. All `DELETE` operations on nodes
. All `MERGE` operations on relationships
. All `DELETE` operations on relationships
====

[#kafka-connect-cypher-strategy]
include::partial$kafka-connect-cypher-strategy.adoc[]

[#kafka-connect-cdc-strategy]
include::partial$kafka-connect-cdc-strategy.adoc[]

[#kafka-connect-pattern-strategy]
include::partial$kafka-connect-pattern-strategy.adoc[]

[#kafka-connect-cud-file-format-strategy]
include::partial$kafka-connect-cud-file-format-strategy.adoc[]

== Multi Database Support

Neo4j Enterprise Edition has https://neo4j.com/docs/operations-manual/current/manage-databases/introduction/[multi-tenancy support].
In order to support this feature with {product-name}, we have to add the `neo4j.database` property to the sink configuration, which tells the Connector the database to use as default.
If you don't specify that property, the home database for the configured user will be used.

Following an example:

[source, json]
----
{
  "name": "Neo4jSinkConnector",
  "config": {
    "neo4j.database": "<database_name>",
    "topics": "topic",
    "connector.class": "streams.kafka.connect.sink.Neo4jSinkConnector",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,
    "errors.retry.timeout": "-1",
    "errors.retry.delay.max.ms": "1000",
    "errors.tolerance": "all",
    "errors.log.enable": true,
    "errors.log.include.messages": true,
    "neo4j.server.uri": "bolt://neo4j:7687",
    "neo4j.authentication.basic.username": "neo4j",
    "neo4j.authentication.basic.password": "password",
    "neo4j.encryption.enabled": false,
    "_comment": "Sink CDC SourceId Strategy",
    "neo4j.topic.cdc.sourceId": "topic",
    "neo4j.topic.cdc.sourceId.labelName": "<the label attached to the node, default=SourceEvent>",
    "neo4j.topic.cdc.sourceId.idName": "<the id name given to the CDC id field, default=sourceId>",
    "_comment": "Sink CDC Schema Strategy",
    "neo4j.topic.cdc.schema": "<list_of_topics_separated_by_semicolon>",
    "_comment": "Sink Node/Relationship Pattern Strategy",
    "neo4j.topic.pattern.node.<TOPIC_NAME>": "<node_extraction_pattern>",
    "neo4j.topic.pattern.relationship.<TOPIC_NAME>": "<relationship_extraction_pattern>",
    "_comment": "Sink CUD File forma Strategy",
    "neo4j.topic.cud": "<list_of_topics_separated_by_semicolon>"
  }
}
----

[#kafka_connect_error_handling]
== How to deal with bad data

In {product-name}, in the creation phase of the Sink instance, in addition to the properties
described in the xref:#dlq-table[Dead Letter Queue configuration parameters table], you need to define kafka broker connection properties:

[[dlq-table]]
.Dead Letter Queue configuration parameters
[%autowidth,cols="m,m,a",opts=header]
|===
| Name | Value | Note
| errors.tolerance | none | fail fast (default!) abort
| errors.tolerance | all | all == lenient, silently ignore bad messages
| errors.log.enable | false/true | log errors (default: false)
| errors.log.include.messages | false/true | log bad messages too (default: false)
| errors.deadletterqueue.topic.name | topic-name | dead letter queue topic name, if left off no DLQ, default: not set
| errors.deadletterqueue.context.headers.enable | false/true | enrich messages with metadata headers like exception, timestamp, org. topic, org.part, default:false
| errors.deadletterqueue.context.headers.prefix | prefix-text | common prefix for header entries, e.g. `"__streams.errors."` , default: not set
| errors.deadletterqueue.topic.replication.factor | 3/1 | replication factor, need to set to 1 for single partition, default:3
|===

For the Neo4j extension you prefix them with `{environment}` in the Neo4j configuration.

|===
| Name | mandatory | Description

| kafka.bootstrap.servers | true | It's the Kafka Broker url. *(please look at the description below)

| kafka.<any_other_kafka_property> | false | You can also specify any other kafka Producer
setting by adding the `kafka.` prefix (i.e the configuration `acks` become `kafka.acks`). See the https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka documentation] for details on these settings.

|===

As you may have noticed we're asking to provide the `bootstrap.server` property,
this because the Kafka Connect Framework provides an out-of-the-box support
only for deserialization errors and message transformations
(please look into the following link for further details: {url-confluent-blog}/kafka-connect-deep-dive-error-handling-dead-letter-queues/).
We want to extend this feature for transient errors in order to cover the 100% of failures.
So to do that at this moment as suggested by Confluent we need to ask again the broker location,
until this JIRA issue will not be addressed: https://issues.apache.org/jira/browse/KAFKA-8597.
Said that, these properties has to be added only if you want to also redirect Neo4j errors into the DLQ.

