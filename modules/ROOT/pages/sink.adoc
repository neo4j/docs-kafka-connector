= Sink Configuration

In this chapter we'll discuss how the Sink instance is configured.

== Create the Sink Instance
:environment: neo4j
:id: neo4j

Sink connector can be configured with different strategies:

* by providing a Cypher template
* by ingesting the events emitted from another Neo4j instance via the Change Data Capture module
* by providing a pattern extraction to a JSON or AVRO file
* by managing a CUD file format

== Sink ingestion strategies

[NOTE]
====
**The Cypher Template strategy is the only Sink strategy that guarantees messages to be processed in the same order as they arrive in a topic.**

Other Sink strategies group messages together by type of operation, which can also be optimised into batches.
In this case, the execution order is the following:

. All `MERGE` operations on nodes
. All `DELETE` operations on nodes
. All `MERGE` operations on relationships
. All `DELETE` operations on relationships
====

[#kafka-connect-cypher-strategy]
include::partial$kafka-connect-cypher-strategy.adoc[]

[#kafka-connect-cdc-strategy]
include::partial$kafka-connect-cdc-strategy.adoc[]

[#kafka-connect-pattern-strategy]
include::partial$kafka-connect-pattern-strategy.adoc[]

[#kafka-connect-cud-file-format-strategy]
include::partial$kafka-connect-cud-file-format-strategy.adoc[]

[#kafka_connect_error_handling]
== How to deal with bad data

In {product-name}, in the creation phase of the Sink instance, in addition to the properties described in the xref:#dlq-table[Dead Letter Queue configuration parameters table], you need to define kafka broker connection properties:

[[dlq-table]]
.Dead Letter Queue configuration parameters
[%autowidth,cols="m,m,a",opts=header]
|===
| Name | Value | Note
| errors.tolerance | none | fail fast (default!) abort
| errors.tolerance | all | all == lenient, silently ignore bad messages
| errors.log.enable | false/true | log errors (default: false)
| errors.log.include.messages | false/true | log bad messages too (default: false)
| errors.deadletterqueue.topic.name | topic-name | dead letter queue topic name, if left off no DLQ, default: not set
| errors.deadletterqueue.context.headers.enable | false/true | enrich messages with metadata headers like exception, timestamp, org. topic, org.part, default:false
| errors.deadletterqueue.context.headers.prefix | prefix-text | common prefix for header entries, e.g. `"__streams.errors."` , default: not set
| errors.deadletterqueue.topic.replication.factor | 3/1 | replication factor, need to set to 1 for single partition, default:3
|===

|===
| Name | mandatory | Description

| kafka.bootstrap.servers | true | It's the Kafka Broker url. *(please look at the description below)

| kafka.<any_other_kafka_property> | false | You can also specify any other kafka Producer
setting by adding the `kafka.` prefix (i.e the configuration `acks` become `kafka.acks`). See the https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka documentation] for details on these settings.

|===

As you may have noticed we're asking to provide the `bootstrap.server` property, this because the Kafka Connect Framework provides an out-of-the-box support only for deserialization errors and message transformations (please look into the following link for further details: {url-confluent-blog}/kafka-connect-deep-dive-error-handling-dead-letter-queues/).
We want to extend this feature for transient errors in order to cover the 100% of failures.
So to do that at this moment as suggested by Confluent we need to ask again the broker location, until this JIRA issue will not be addressed: https://issues.apache.org/jira/browse/KAFKA-8597.
Said that, these properties has to be added only if you want to also redirect Neo4j errors into the DLQ.

