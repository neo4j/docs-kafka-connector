[#kafka_connect_error_handling]
= Error handling

{product-name} sink instance provides an error handling mechanism to deal with bad incoming data.
In order to support this feature, we need to define the properties described in the xref:#dlq-table[Dead Letter Queue configuration parameters table]:

[[dlq-table]]
.Dead Letter Queue configuration parameters
[%autowidth,cols="m,a",opts=header]
|===
| Name | Description
| errors.tolerance | One of `none`: fail fast, `all`: ignore bad messages

Default: `none`

| errors.log.enable | One of `true`, `false`.
Log errors

Default: `false`

| errors.log.include.messages | One of `true`, `false`.

Include details (the topic, partition, offset, and timestamp) in the logs

Default: `false`

| errors.deadletterqueue.topic.name | Dead letter queue topic name.

Default: no dead letter queue

| errors.deadletterqueue.context.headers.enable | One of `true`, `false`.
Enables error headers

Default: `false`

| errors.deadletterqueue.topic.replication.factor | Replication factor.
Set 1 for single partition

Default: `3`
|===

== errors.tolerance

Configures error tolerance during the sink process.
The default value is `none`, causing any error to immediately fail the connector task and stop processing the remaining messages.
Setting it to `all` changes the behavior to skip problematic messages and continue processing the rest.

[NOTE]
If `error.tolerance: all`, problematic messages will still be reported if a dead letter queue is defined.

== errors.log.enable

If set to true, each error, along with details of the failed operation and problematic message, will be written to the Kafka Connect application log.
By default, this is set to 'false', so only errors that are not tolerated are reported.

== errors.log.include.messages

Specifies whether to include the Kafka message that resulted in a failure in the log.
If enabled, the topic, partition, offset, and timestamp will be logged.
By default, this is set to 'false', preventing message keys, values, and headers from being written to log files.

== errors.deadletterqueue.topic.name

Specifies the topic name to be used as the dead letter queue (DLQ) for messages that encounter errors during the sink process.
When a topic name is set, failed messages will be sent to the DLQ.
By default, the topic name is blank, indicating that no messages will be sent to the DLQ.

== errors.deadletterqueue.context.headers.enable

If set to true, headers containing error context will be added to the messages sent to the DLQ topic.
To prevent conflicts with headers from the original record, all error context header keys will start with `__connect.errors`.
The error headers that will be sent;

* `__connect.errors.topic`
* `__connect.errors.partition`
* `__connect.errors.offset`
* `__connect.errors.connector.name`
* `__connect.errors.task.id`
* `__connect.errors.stage`
* `__connect.errors.class.name`
* `__connect.errors.exception.class.name`
* `__connect.errors.exception.message`
* `__connect.errors.exception.stacktrace`

== errors.deadletterqueue.topic.replication.factor

Specifies the replication factor used to create the dead letter queue (DLQ) topic if it does not already exist.
Default value `3`, If youâ€™re running on a single-node Kafka cluster, you will also need to set it to `1`.

== Example Configuration FLow

This flowchart shows how to choose which one to use:

image::sink-error-handling-flow.png[title="Configuration Flowchart", align="center"]

[NOTE]
With this version of the connector, we also handle errors that may occur during message processing and when writing messages to the target Neo4j database.
For further details about how Kafka Connect Framework handles error management, please look into the following link: https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/
