=== Cypher Strategy

This strategy executes corresponding Cypher statements for each message received.

To configure a cypher strategy for a desired topic, you must follow the following convention:

[source,json,subs="verbatim,attributes"]
----
"neo4j.topic.cypher.<YOUR_TOPIC>": "<YOUR_CYPHER_QUERY>"
----

==== Example

Given that you configure the topics your sink connector subscribes to within the sink configuration settings as follows;

[source,json]
----
  "topics": "creates,updates,deletes"
----

You need to declare that you want to use `cypher` strategy and provide the corresponding Cypher statement for each topic, similar to the following;

[source,json]
----
  "topics": "creates,updates,deletes",
  "neo4j.topic.cypher.creates": "WITH event AS message WITH message.event.state.after AS state MERGE (p:Person {name: state.properties.name, surname: state.properties.surname}) MERGE (f:Family {name: state.properties.surname}) MERGE (p)-[:BELONGS_TO]->(f)",
  "neo4j.topic.cypher.updates": "WITH event AS message WITH message.event.state.before AS before, message.event.state.after AS after MATCH (p:Person {name: before.properties.name, surname: before.properties.surname}) MATCH (fPre:Family {name: before.properties.surname}) OPTIONAL MATCH (p)-[b:BELONGS_TO]->(fPre) DELETE b WITH after, p SET p.name = after.properties.name, p.surname = after.properties.surname MERGE (f:Family {name: after.properties.surname}) MERGE (p)-[:BELONGS_TO]->(f)",
  "neo4j.topic.cypher.deletes": "WITH event AS message WITH message.event.state.before AS before MATCH (p:Person {name: before.properties.name, surname: before.properties.surname}) DETACH DELETE p"
----

The above configuration excerpt defines that;

* messages received from `creates` topic will be unpacked by the sink connector into Neo4j with the following Cypher query:
+
[source,cypher]
----
WITH event AS message
WITH message.event.state.after AS state
MERGE (p:Person {name: state.properties.name, surname: state.properties.surname})
MERGE (f:Family {name: state.properties.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

* messages received from `updates` topic will be unpacked by the sink connector into Neo4j with the following Cypher query:
+
[source,cypher]
----
WITH event AS message
WITH message.event.state.before AS before, message.event.state.after AS after
MATCH (p:Person {name: before.properties.name, surname: before.properties.surname})
MATCH (fPre:Family {name: before.properties.surname})
OPTIONAL MATCH (p)-[b:BELONGS_TO]->(fPre)
DELETE b
WITH after, p
SET p.name = after.properties.name, p.surname = after.properties.surname
MERGE (f:Family {name: after.properties.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

* messages received from `deletes` topic will be unpacked by the sink connector into Neo4j with the following Cypher query:
+
[source,cypher]
----
WITH event AS message
WITH message.event.state.before AS before
MATCH (p:Person {name: before.properties.name, surname: before.properties.surname})
DETACH DELETE p
----

===== Creating Sink Instance

Based on the above example, we can use one of the following configurations.
Pick one of the following message serialization formats, save the content of the provided file into a local directory, named as `sink.cypher.neo4j.json`.

[.tabbed-example]
====
[.include-with-AVRO-messages]
=====
[source,json]
----
include::ROOT:example$docker-data/contrib.source.avro.neo4j.json[]
----
=====

[.include-with-JSON-messages-with-schema]
=====
[source,json]
----
include::ROOT:example$docker-data/contrib.source.json.neo4j.json[]
----
=====

[.include-with-JSON-messages-as-string]
=====
[source,json]
----
include::ROOT:example$docker-data/contrib.source.json-string.neo4j.json[]
----
=====
====

Let's load the configuration into the Kafka Connect with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @sink.cypher.neo4j.json
----

Now you can access your Confluent Control Center instance under: \http://localhost:9021/clusters, and verify that the configured connector instance is running under Connect, connect-default.

[NOTE]
====
Under the hood the connector will create a batch of changes for each topic, and will execute the query with a prepended `UNWIND` clause.
For the above example, the executed query for messages received from `creates` topic would look like;

[source,cypher]
----
UNWIND $events AS event
WITH event AS message
WITH message.event.state.after AS state
MERGE (p:Person {name: state.properties.name, surname: state.properties.surname})
MERGE (f:Family {name: state.properties.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

where `$events` is a batch of change events.
====