=== Cypher Strategy

This strategy executes corresponding Cypher statements for each message received.

To configure a cypher strategy for a desired topic, you must follow the following convention:

[source,json,subs="verbatim,attributes"]
----
"neo4j.cypher.topic.<YOUR_TOPIC>": "<YOUR_CYPHER_QUERY>"
----

[IMPORTANT]
====
Starting with version 5.1.0 of the {product-name}, the Cypher strategy binds header, key, and value of the messages as `\__header`, `__key`, and `__value` respectively, which are passed to the user-provided Cypher query as predefined variables.
See xref:cypher_strategy_settings[] for more information on how to customize the name of the variables.

For backward compatibility, `event` is still available for use but will only correspond to the value of the message.
====

==== Example

Given that you configure the topics your sink connector subscribes to within the sink configuration settings as follows;

[source,json]
----
  "topics": "creates,updates,deletes"
----

You need to declare that you want to use `cypher` strategy and provide the corresponding Cypher statement for each topic, similar to the following;

[source,json]
----
  "topics": "creates,updates,deletes",
  "neo4j.cypher.topic.creates": "WITH __value.event.state.after AS state MERGE (p:Person {name: state.properties.name, surname: state.properties.surname}) MERGE (f:Family {name: state.properties.surname}) MERGE (p)-[:BELONGS_TO]->(f)",
  "neo4j.cypher.topic.updates": "WITH __value.event.state.before AS before, __value.event.state.after AS after MATCH (p:Person {name: before.properties.name, surname: before.properties.surname}) MATCH (fPre:Family {name: before.properties.surname}) OPTIONAL MATCH (p)-[b:BELONGS_TO]->(fPre) DELETE b WITH after, p SET p.name = after.properties.name, p.surname = after.properties.surname MERGE (f:Family {name: after.properties.surname}) MERGE (p)-[:BELONGS_TO]->(f)",
  "neo4j.cypher.topic.deletes": "WITH __value.event.state.before AS before MATCH (p:Person {name: before.properties.name, surname: before.properties.surname}) DETACH DELETE p"
----

The above configuration excerpt defines that;

* messages received from `creates` topic will be unpacked by the sink connector into Neo4j with the following Cypher query:
+
[source,cypher]
----
WITH __value.event.state.after AS state
MERGE (p:Person {name: state.properties.name, surname: state.properties.surname})
MERGE (f:Family {name: state.properties.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

* messages received from `updates` topic will be unpacked by the sink connector into Neo4j with the following Cypher query:
+
[source,cypher]
----
WITH __value.event.state.before AS before, __value.event.state.after AS after
MATCH (p:Person {name: before.properties.name, surname: before.properties.surname})
MATCH (fPre:Family {name: before.properties.surname})
OPTIONAL MATCH (p)-[b:BELONGS_TO]->(fPre)
DELETE b
WITH after, p
SET p.name = after.properties.name, p.surname = after.properties.surname
MERGE (f:Family {name: after.properties.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

* messages received from `deletes` topic will be unpacked by the sink connector into Neo4j with the following Cypher query:
+
[source,cypher]
----
WITH __value.event.state.before AS before
MATCH (p:Person {name: before.properties.name, surname: before.properties.surname})
DETACH DELETE p
----

===== Creating Sink Instance

Based on the above example, we can use one of the following configurations.
Pick one of the following message serialization formats, save the content of the provided file into a local directory, named as `sink.cypher.neo4j.json`.

[.tabbed-example]
====
[.include-with-AVRO-messages]
=====
ifdef::backend-pdf[]
.sink.cypher.avro.json
endif::[]
[source,json]
----
include::example$docker-data/sink.cypher.avro.json[]
----
=====

[.include-with-JSON-messages-with-schema]
=====
ifdef::backend-pdf[]
.sink.cypher.json.json
endif::[]
[source,json]
----
include::example$docker-data/sink.cypher.json.json[]
----
=====

[.include-with-PROTOBUF-messages]
=====
ifdef::backend-pdf[]
.sink.cypher.protobuf.json
endif::[]
[source,json]
----
include::example$docker-data/sink.cypher.protobuf.json[]
----
=====

====

Let's load the configuration into the Kafka Connect with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @sink.cypher.neo4j.json
----

Now you can access your Confluent Control Center instance under: \http://localhost:9021/clusters, and verify that the configured connector instance is running under Connect, connect-default.

[NOTE]
====
Under the hood the connector will create a batch of changes for each topic, and will execute the query with a prepended `UNWIND` clause.
For the above example, the executed query for messages received from `creates` topic would look like;

[source,cypher]
----
UNWIND $events AS message
WITH message.value AS event, message.header AS __header, message.key AS __key, message.value AS __value
WITH __value.event.state.after AS state
MERGE (p:Person {name: state.properties.name, surname: state.properties.surname})
MERGE (f:Family {name: state.properties.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

where `$events` is a batch of change events.
====